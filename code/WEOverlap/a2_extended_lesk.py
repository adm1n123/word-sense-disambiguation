# -*- coding: utf-8 -*-
"""A2_Extended_LESK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEqadjjvu7nj2eaAHVDxR-5LMtbHqF31
"""

!pip install nltk==3.5

from gensim.models import KeyedVectors
w2v_vec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/CS772/A1/wiki-news-300d-1M.vec')

import nltk
nltk.download('semcor')
from nltk.corpus import semcor
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk import word_tokenize
import numpy as np
from numpy import dot
from numpy.linalg import norm
from nltk.corpus import wordnet as wn
from sklearn.metrics import classification_report, precision_recall_fscore_support

MAX_SENSES = 5

def get_sense_dict(sents):
    sense_set = set()
    for i in range(len(sents)):
        for j in range(len(sents[i])):
            if isinstance(sents[i][j], nltk.Tree):
                try :
                    if sents[i][j].height() == 3:
                        for tree in sents[i][j]:
                            if(tree.label() == 'NE'):
                                sense = 'NE'
                                sense_set.add(sense) 
                    else :
                        sense = sents[i][j].label().synset().name()
                        sense_set.add(sense)
 
                except:
                    if(sents[i][j].label() == 'NE'):
                        sense = sents[i][j].label()
                        sense_set.add(sense)
                    else :
                        sense = sents[i][j].label()
                        sense_set.add(sense)
            else :
                sense = 'notag'
                sense_set.add(sense)
    sense_set.add('unk')
    sense_set = list(sense_set)
    sense_dict = {sense : i for i, sense in enumerate(sense_set)}
    return sense_dict

def extended_lesk(word, sentence) :
    word_synsets = wn.synsets(word)
    # word_synsets = wn.synsets(word)[:MAX_SENSES]
    if(len(word_synsets) == 0):
        return 'notag'
    
    word_synsets += wn.synset(word_synsets[0].name()).hyponyms()[:MAX_SENSES]
    sense_definitions = []
    for synset in word_synsets:
      try:
        sense_definitions.append(synset.definition() + synset.examples()[0])
      except:
        sense_definitions.append(synset.definition())


    def_emb = []
    stopword_list = stopwords.words("english")
    for i in range(len(sense_definitions)):
        words = word_tokenize(sense_definitions[i])
        words = [token for token in words if(token not in stopword_list and token != word)]
        embedding = np.array([0.0] * 300)
        num_of_words = len(words)
        for curr_word in words:
            if curr_word in w2v_vec.vocab:
                embedding += w2v_vec[curr_word]
            else:
                num_of_words -= 1
        embedding = embedding / max(num_of_words, 1)
        def_emb.append(embedding)

    context = [token for token in sentence if(token not in stopword_list and token != word)]
    context_emb = np.array([0.0] * 300)
    context_size = len(context)
    for context_word in context:
        if context_word in w2v_vec.vocab:
            context_emb += w2v_vec[context_word]
        else:
            context_size -= 1
    context_emb = context_emb / max(context_size, 1)

    cos_sim = []
    for i in range(len(sense_definitions)):
        curr = dot(context_emb, def_emb[i])/(norm(context_emb)*norm(def_emb[i]))
        cos_sim.append(curr) 
    ind = np.argmax(cos_sim)
    return word_synsets[ind].name()

def predict(sents, untagged_sents):
    pred_sense = []
    actual_sense = []
    for i in range(len(sents)):
        for j in range(len(sents[i])):
            if isinstance(sents[i][j], nltk.Tree):
                try :
                    if sents[i][j].height() == 3:
                        for tree in sents[i][j]:
                            if(tree.label() == 'NE'):
                                sense = 'NE'
                                actual_sense.append(sense)
                                word = "_".join(tree.leaves())
                    else :
                        sense = sents[i][j].label().synset().name()
                        actual_sense.append(sense)
                        word = "_".join(sents[i][j].leaves())
                    pred_sense.append(extended_lesk(word, untagged_sents[i]))                   
                except:
                    if(sents[i][j].label() == 'NE'):
                        sense = sents[i][j].label()
                        word = "_".join(sents[i][j].leaves())
                        actual_sense.append(sense)
                    else :
                        sense = sents[i][j].label()
                        word = "_".join(sents[i][j].leaves())
                        actual_sense.append(sense)
                    pred_sense.append(extended_lesk(word, untagged_sents[i]))                
            else :
                sense = 'notag'
                actual_sense.append(sense)
                word = "_".join(sents[i][j])
                pred_sense.append(extended_lesk(word, untagged_sents[i]))           
    return actual_sense, pred_sense

sents = semcor.tagged_sents(tag='sem')
sense_dict = get_sense_dict(sents)

sents = semcor.tagged_sents(tag='sem')[150:155]
untagged_sents = semcor.sents()[150:155]
y_true, y_pred = predict(sents, untagged_sents)
extra_senses = set(sense_dict.keys())
for sense in y_pred:
    extra_senses.add(sense)
sense_dict = {sense : i for i, sense in enumerate(extra_senses)}
k = 0
for i in range(5):
  print(i)
  for j in range(len(sents[i])):
      try:
        print(sents[i][j].leaves(), y_true[k], y_pred[k]) 
      except:
        print(sents[i][j], y_true[k], y_pred[k]) 
      k += 1

y_true = [sense_dict[sense] for sense in y_true]
y_pred = [sense_dict[sense] for sense in y_pred]

sents = semcor.tagged_sents(tag='sem')
untagged_sents = semcor.sents()
y_true, y_pred = predict(sents, untagged_sents)
extra_senses = set(sense_dict.keys())
for sense in y_pred:
    extra_senses.add(sense)
sense_dict = {sense : i for i, sense in enumerate(extra_senses)}
y_true = [sense_dict[sense] for sense in y_true]
y_pred = [sense_dict[sense] for sense in y_pred]

print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))
print(precision_recall_fscore_support(y_true, y_pred, average='weighted', beta=0.5))
print(precision_recall_fscore_support(y_true, y_pred, average='weighted', beta=2))

sents = semcor.tagged_sents(tag='sem')
untagged_sents = semcor.sents()
y_true, y_pred = predict(sents, untagged_sents)

y_true = [sense_dict[sense] for sense in y_true]
y_pred = [sense_dict.get(sense, sense_dict['notag']) for sense in y_pred]

print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))
print(precision_recall_fscore_support(y_true, y_pred, average='weighted', beta=0.5))
print(precision_recall_fscore_support(y_true, y_pred, average='weighted', beta=2))